{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JumanaWanass/NER-using-HMM-and-MEMM/blob/main/Named_Entity_Recognition_MEMM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEyk2ch1GYi0"
      },
      "source": [
        "### Data preprocessing\n",
        "\n",
        "In this section we will write code to load data and build the dataset for Named Entity Recognition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_hx0PKdII9C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import codecs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmssMqR2IKkC"
      },
      "source": [
        "Write a function to load sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4MKd94oHDKr"
      },
      "outputs": [],
      "source": [
        "def zero_digits(s):\n",
        "    \"\"\"\n",
        "    Replace all digits in a string with zeros.\n",
        "    \"\"\"\n",
        "    return re.sub('\\d', '0', s)\n",
        "\n",
        "def load_sentences(path):\n",
        "    \"\"\"\n",
        "    Load sentences. A line must contain at least a word and its tag.\n",
        "    Sentences are separated by empty lines.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    for line in codecs.open(path, 'r', 'utf8'):\n",
        "        line = zero_digits(line.rstrip())\n",
        "        if not line:\n",
        "            if len(sentence) > 0:\n",
        "                if 'DOCSTART' not in sentence[0][0]:\n",
        "                    sentences.append(sentence)\n",
        "                sentence = []\n",
        "        else:\n",
        "            word = line.split()\n",
        "            assert len(word) >= 2\n",
        "            sentence.append(word)\n",
        "    if len(sentence) > 0:\n",
        "        if 'DOCSTART' not in sentence[0][0]:\n",
        "            sentences.append(sentence)\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W76Clg4J2wo"
      },
      "source": [
        "Prepare the training/test data using the loaded sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yS9JsVSMZ6l"
      },
      "outputs": [],
      "source": [
        "def create_dico(item_list):\n",
        "    \"\"\"\n",
        "    Create a dictionary of items from a list of list of items.\n",
        "    \"\"\"\n",
        "    assert type(item_list) is list\n",
        "    dico = {}\n",
        "    for items in item_list:\n",
        "        for item in items:\n",
        "            if item not in dico:\n",
        "                dico[item] = 1\n",
        "            else:\n",
        "                dico[item] += 1\n",
        "    return dico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_7t9tVCMdFY"
      },
      "outputs": [],
      "source": [
        "def create_mapping(dico):\n",
        "    \"\"\"\n",
        "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
        "    Items are ordered by decreasing frequency.\n",
        "    \"\"\"\n",
        "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
        "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items) if v[1] > 2}\n",
        "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
        "    return item_to_id, id_to_item\n",
        "\n",
        "def word_mapping(sentences):\n",
        "    \"\"\"\n",
        "    Create a dictionary and a mapping of words, sorted by frequency.\n",
        "    \"\"\"\n",
        "    words = [[x[0] for x in s] for s in sentences]\n",
        "    dico = create_dico(words)\n",
        "    dico['<UNK>'] = 10000000\n",
        "    word_to_id, id_to_word = create_mapping(dico)\n",
        "    print(\"Found %i unique words (%i in total)\" % (\n",
        "        len(dico), sum(len(x) for x in words))\n",
        "    )\n",
        "    return dico, word_to_id, id_to_word\n",
        "\n",
        "def tag_mapping(sentences):\n",
        "    \"\"\"\n",
        "    Create a dictionary and a mapping of tags, sorted by frequency.\n",
        "    \"\"\"\n",
        "    tags = [[word[-1] for word in s] for s in sentences]\n",
        "    dico = create_dico(tags)\n",
        "    tag_to_id, id_to_tag = create_mapping(dico)\n",
        "    print(\"Found %i unique named entity tags\" % len(dico))\n",
        "    return dico, tag_to_id, id_to_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNS57L87IQdT"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(sentences, mode=None, word_to_id=None, tag_to_id=None):\n",
        "    \"\"\"\n",
        "    Prepare the dataset. Return 'data', which is a list of lists of dictionaries containing:\n",
        "        - words (strings)\n",
        "        - word indexes\n",
        "        - tag indexes\n",
        "    \"\"\"\n",
        "    assert mode == 'train' or (mode == 'test' and word_to_id and tag_to_id)\n",
        "\n",
        "    if mode=='train':\n",
        "        word_dic, word_to_id, id_to_word = word_mapping(sentences)\n",
        "        tag_dic, tag_to_id, id_to_tag = tag_mapping(sentences)\n",
        "\n",
        "    def f(x): return x\n",
        "    data = []\n",
        "    for s in sentences:\n",
        "        str_words = [w[0] for w in s]\n",
        "        words = [word_to_id[f(w) if f(w) in word_to_id else '<UNK>']\n",
        "                 for w in str_words]\n",
        "        tags = [tag_to_id[w[-1]] for w in s]\n",
        "        data.append({\n",
        "            'str_words': str_words,\n",
        "            'words': words,\n",
        "            'tags': tags,\n",
        "        })\n",
        "\n",
        "    if mode == 'train':\n",
        "        return data, {'word_to_id':word_to_id, 'id_to_word':id_to_word, 'tag_to_id':tag_to_id, 'id_to_tag':id_to_tag}\n",
        "    else:\n",
        "        return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgikvX-qKTL7"
      },
      "source": [
        "### Maximum Entropy Markov Model\n",
        "In this section, we will implement a maximum entropy markov model (MEMM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te89fFIgLdFf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn import linear_model\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from scipy import sparse\n",
        "import collections\n",
        "import codecs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hARYFWRZwDGk"
      },
      "source": [
        "## Implement MEMM extract_feature for problem 2(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhQXE7CHHNy6"
      },
      "outputs": [],
      "source": [
        "class MEMM(object):\n",
        "    \"\"\"\n",
        "     MEMM Model\n",
        "    \"\"\"\n",
        "    def __init__(self, dic, decode_type, regex_features, rear_context_window=4, forward_context_window=4, num_prior_tags=6):\n",
        "        \"\"\"\n",
        "        Initialize the model.\n",
        "        \"\"\"\n",
        "\n",
        "        self.num_words = len(dic['word_to_id'])\n",
        "        self.num_tags = len(dic['tag_to_id'])\n",
        "\n",
        "        self.rear_context_window = rear_context_window\n",
        "        self.forward_context_window = forward_context_window\n",
        "        self.num_prior_tags = num_prior_tags\n",
        "\n",
        "        self.decode_type = decode_type\n",
        "        tag_labels = list(dic['id_to_tag'].keys()) + [-1]  # add [-1] for out of bounds positions\n",
        "        self.tag_encoder = OneHotEncoder(handle_unknown='error', sparse=False).fit(np.array(tag_labels).reshape(-1, 1))\n",
        "        self.model = linear_model.LogisticRegression(penalty='l2', C=0.5, verbose=True, solver='lbfgs', max_iter=500, n_jobs=-1)  # MaxEnt model\n",
        "        self.regular_expressions = collections.OrderedDict(regex_features.items())\n",
        "        self.dic = dic\n",
        "        self.word_features_cache = dict()\n",
        "        self.tag_features_cache = dict()\n",
        "        return\n",
        "\n",
        "    def check_regex(self, pattern, word_str):\n",
        "        return re.search(pattern, word_str) is not None\n",
        "\n",
        "    def extract_feature(self, words, tags, i):\n",
        "        \"\"\"\n",
        "        TODO: Complete the extract_feature to include tag features from prior words\n",
        "        Extract word and tag features to predict i'th tag\n",
        "\n",
        "        Args:\n",
        "            words: dict, consisting of {k: k'th word in sentence}\n",
        "            tags: dict, consisting of {k: k'th tag in sentence}\n",
        "            i: the index of the tag we want to predict with this feature\n",
        "        \"\"\"\n",
        "\n",
        "        # Get features for each word in the context window\n",
        "        window = [words.get(i_x, -1) for i_x in range(i - self.rear_context_window, i + self.forward_context_window + 1)]  # -1 for words out of bounds\n",
        "        if tuple(window) in self.word_features_cache:  # caching speeds up feature extraction a bit\n",
        "            word_features = self.word_features_cache[tuple(window)]\n",
        "        else:\n",
        "            word_strs = list(map(lambda word_id: self.dic['id_to_word'].get(word_id, ''), window))\n",
        "            word_features = list()\n",
        "            for word in word_strs:\n",
        "                for pattern in self.regular_expressions.values():\n",
        "                    word_features.append(self.check_regex(pattern, word))\n",
        "        self.word_features_cache[tuple(window)] = word_features\n",
        "\n",
        "        prior_tags = list()\n",
        "        # TODO: Set prior_tags to the list of tag ids for the last (self.num_prior_tags) tags\n",
        "        # (6 points)\n",
        "        # START HERE\n",
        "        prior_tags = [tags[i - j - 1] for j in range(self.num_prior_tags) if i - j - 1 >= 0]\n",
        "        # END\n",
        "\n",
        "        if tuple(prior_tags) in self.tag_features_cache:\n",
        "            tag_features = self.tag_features_cache[tuple(prior_tags)]\n",
        "        else:\n",
        "            tag_features =  list()\n",
        "            # TODO: Add one-hot encoding features to tag_features\n",
        "            # (6 points)\n",
        "            # START HERE\n",
        "            tag_features = self.tag_encoder.transform(np.array(prior_tags).reshape(-1, 1)).toarray()\n",
        "            # END\n",
        "\n",
        "        feature = np.append(word_features, tag_features)\n",
        "        self.tag_features_cache[tuple(prior_tags)] = tag_features\n",
        "        return feature.reshape(1, -1)\n",
        "\n",
        "    def get_features_labels(self, sentence):\n",
        "        \"\"\"\n",
        "        Returns the features and labels for each tag in a sentence.\n",
        "        \"\"\"\n",
        "        words = dict(enumerate(sentence['words']))\n",
        "        tags = dict(enumerate(sentence['tags']))\n",
        "        features = list()\n",
        "        labels = list()\n",
        "        for i in range(0, len(tags)):\n",
        "                feature = self.extract_feature(words, tags, i).flatten()\n",
        "                label = tags[i]\n",
        "                features.append(feature)\n",
        "                labels.append(label)\n",
        "        return features, labels\n",
        "\n",
        "    def train(self, corpus):\n",
        "        \"\"\"\n",
        "        Train an MEMM model using MLE estimates.\n",
        "\n",
        "        Args:\n",
        "            corpus is a list of dictionaries of the form:\n",
        "            {'str_words': str_words,   ### List of string words\n",
        "            'words': words,            ### List of word IDs\n",
        "            'tags': tags}              ### List of tag IDs\n",
        "            All three lists above have length equal to the sentence length for each instance.\n",
        "        \"\"\"\n",
        "\n",
        "        X = list()\n",
        "        y = list()\n",
        "        print('Extracting features...')\n",
        "        for sentence in tqdm(corpus):\n",
        "            features, labels = self.get_features_labels(sentence)\n",
        "            X.extend(features)\n",
        "            y.extend(labels)\n",
        "        print('Training MaxEnt model. This usually finishes within 1-3 minutes.')\n",
        "        self.model.fit(X, y)\n",
        "\n",
        "        return\n",
        "\n",
        "    def greedy_decode(self, sentence):\n",
        "        \"\"\"\n",
        "        Decode a single sentence in Greedy fashion\n",
        "        Return a list of tags.\n",
        "        \"\"\"\n",
        "        words = dict(enumerate(sentence))\n",
        "        y_tags = dict()  # stores past tags\n",
        "        for i in range(0, len(sentence)):\n",
        "            feature = self.extract_feature(words, y_tags, i)\n",
        "            y_hat = np.argmax(self.model.predict_proba(feature)).item()\n",
        "            y_tags[i] = y_hat\n",
        "\n",
        "        tags = [y_tags[i] for i in range(len(sentence))]\n",
        "        assert len(tags) == len(sentence)\n",
        "        return tags\n",
        "\n",
        "    def tag(self, sentence):\n",
        "        \"\"\"\n",
        "        Tag a sentence using a trained MEMM.\n",
        "        \"\"\"\n",
        "        return self.greedy_decode(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6P0Qs5JM0x3"
      },
      "source": [
        "### Train and evaluate MEMMs.\n",
        "This section contains driver code for learning an MEMM for named entity recognition on a training corpus and evaluating it on a test corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJRRLA2jOsl1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import codecs\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import collections\n",
        "from sklearn.metrics import f1_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk2KF59LOzSr"
      },
      "source": [
        "Write a function to tag the test corpus with a trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldGSAHWSOwt1"
      },
      "outputs": [],
      "source": [
        "def tag_corpus(model, test_corpus, output_file, dic):\n",
        "    if output_file:\n",
        "        f_output = codecs.open(output_file, 'w', 'utf-8')\n",
        "    start = time.time()\n",
        "\n",
        "    num_correct = 0.\n",
        "    num_total = 0.\n",
        "    y_pred=[]\n",
        "    y_actual=[]\n",
        "    print('Tagging...')\n",
        "    for i, sentence in enumerate(tqdm(test_corpus)):\n",
        "        tags = model.tag(sentence['words'])\n",
        "        str_tags = [dic['id_to_tag'][t] for t in tags]\n",
        "        y_pred.extend(tags)\n",
        "        y_actual.extend(sentence['tags'])\n",
        "\n",
        "        # Check accuracy.\n",
        "        num_correct += np.sum(np.array(tags) == np.array(sentence['tags']))\n",
        "        num_total += len([w for w in sentence['words']])\n",
        "\n",
        "        if output_file:\n",
        "            f_output.write('%s\\n' % ' '.join('%s%s%s' % (w, '__', y)\n",
        "                                             for w, y in zip(sentence['str_words'], str_tags)))\n",
        "\n",
        "    print('---- %i lines tagged in %.4fs ----' % (len(test_corpus), time.time() - start))\n",
        "    if output_file:\n",
        "        f_output.close()\n",
        "\n",
        "    print(\"Overall accuracy: %s\\n\" % (num_correct/num_total))\n",
        "    return y_pred,y_actual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhAjb7OuPEgT"
      },
      "source": [
        "Write a function a compute the confusion matrix and the F-1 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KC_VhUG0O98M"
      },
      "outputs": [],
      "source": [
        "def compute_score(y_pred,y_actual):\n",
        "    A = confusion_matrix(y_actual, y_pred)\n",
        "    f1 = f1_score(y_actual, y_pred,average=None)\n",
        "    print(\"Confusion Matrix:\\n\", A)\n",
        "    print(\"F-1 scores: \", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLFBiD4BPMl1"
      },
      "source": [
        "Write a function to train and evalute the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFcYYThhPRbh"
      },
      "outputs": [],
      "source": [
        "def runMEMM(train_corpus,\n",
        "            test_corpus,\n",
        "            dic,\n",
        "            decode_type,\n",
        "            regex_features,\n",
        "            rear_context_window,\n",
        "            forward_context_window,\n",
        "            num_prior_tags,\n",
        "            output_file):\n",
        "    # build and train the model\n",
        "    model = MEMM(dic, decode_type, regex_features=regex_features,\n",
        "                 rear_context_window=rear_context_window,\n",
        "                 forward_context_window=forward_context_window, num_prior_tags=num_prior_tags)\n",
        "    model.train(train_corpus)\n",
        "\n",
        "    print(\"Train results:\")\n",
        "    pred, real = tag_corpus(model, train_corpus, output_file, dic)\n",
        "\n",
        "    print(\"Tags: \", dic['id_to_tag'])\n",
        "    A = compute_score(pred,real)\n",
        "\n",
        "    # test on validation\n",
        "    print(\"\\n-----------\\nTest results:\")\n",
        "    pred, real = tag_corpus(model, test_corpus, output_file, dic)\n",
        "\n",
        "    print(\"Tags: \", dic['id_to_tag'])\n",
        "    A = compute_score(pred,real)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McZ9KpeQRlA9"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjVwUkR9Rq53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "241613f4-4005-4c5b-d348-518ee2020ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-21 14:41:20--  https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\n",
            "Resolving princeton-nlp.github.io (princeton-nlp.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to princeton-nlp.github.io (princeton-nlp.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3213441 (3.1M) [application/octet-stream]\n",
            "Saving to: ‘eng.train.1’\n",
            "\n",
            "\reng.train.1           0%[                    ]       0  --.-KB/s               \reng.train.1         100%[===================>]   3.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-03-21 14:41:20 (223 MB/s) - ‘eng.train.1’ saved [3213441/3213441]\n",
            "\n",
            "--2024-03-21 14:41:20--  https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\n",
            "Resolving princeton-nlp.github.io (princeton-nlp.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to princeton-nlp.github.io (princeton-nlp.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 774436 (756K) [application/octet-stream]\n",
            "Saving to: ‘eng.val.1’\n",
            "\n",
            "eng.val.1           100%[===================>] 756.29K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2024-03-21 14:41:20 (127 MB/s) - ‘eng.val.1’ saved [774436/774436]\n",
            "\n",
            "Found 20101 unique words (203621 in total)\n",
            "Found 5 unique named entity tags\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!wget https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\n",
        "!wget https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\n",
        "\n",
        "train_file = 'eng.train'\n",
        "test_file = 'eng.val'\n",
        "\n",
        "# Load the training data\n",
        "train_sentences = load_sentences(train_file)\n",
        "train_corpus, dic = prepare_dataset(train_sentences, mode='train', word_to_id=None, tag_to_id=None)\n",
        "\n",
        "# Load the testing data\n",
        "test_sentences = load_sentences(test_file)\n",
        "test_corpus = prepare_dataset(test_sentences, mode='test', word_to_id=dic['word_to_id'], tag_to_id=dic['tag_to_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0xJ_MYGyZkW"
      },
      "source": [
        "## (!!) Visualize the dataset\n",
        "\n",
        "To help in devising useful features for MEMM models, it can be useful to inspect the dataset to see prevailing patterns among different entity tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLqXU8cMyN3G"
      },
      "outputs": [],
      "source": [
        "# Get frequencies for common words and their respective tag.\n",
        "tag_word_examples = {key: collections.Counter() for key in dic['tag_to_id'].keys()}\n",
        "for sentence in train_corpus:\n",
        "    for word, tag in zip(sentence['str_words'], sentence['tags']):\n",
        "        tag_word_examples[dic['id_to_tag'][tag]][word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SRCEM4HydqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "674342e1-eb9b-4e11-ac57-c233a0ea10ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O: [('.', 7362), (',', 7275), ('the', 7221), ('0', 6162), ('of', 3617), ('00', 3582), ('in', 3390), ('to', 3382), ('a', 2994), ('(', 2846), (')', 2846), ('and', 2789), ('\"', 2178), ('on', 2036), ('said', 1846), (\"'s\", 1496), ('for', 1406), ('-', 1243), ('was', 1095), ('The', 1093)]\n",
            "PER: [('Clinton', 93), ('Mark', 58), ('Michael', 55), ('Paul', 51), ('John', 50), ('David', 50), ('Ahmed', 49), ('Arafat', 47), ('Martin', 47), ('Yeltsin', 47), ('M.', 46), ('Dole', 44), ('Lebed', 42), ('Akram', 37), ('Dutroux', 36), ('Wasim', 34), ('Thomas', 34), ('A.', 32), ('Peter', 31), ('Robert', 29)]\n",
            "ORG: [('of', 104), ('Reuters', 79), ('Newsroom', 70), ('Inc', 61), ('St', 52), ('Party', 49), ('New', 49), ('Corp', 49), ('National', 48), ('United', 48), ('and', 46), ('Commission', 45), ('Bank', 43), ('Union', 40), ('U.N.', 40), ('Co', 38), (\"'s\", 34), ('Ajax', 34), ('Sydney', 34), ('York', 33)]\n",
            "LOC: [('U.S.', 309), ('Germany', 142), ('Britain', 133), ('Australia', 130), ('England', 124), ('France', 122), ('Spain', 110), ('Italy', 98), ('New', 95), ('LONDON', 93), ('China', 91), ('Russia', 88), ('Japan', 87), ('Pakistan', 85), ('Sweden', 81), ('South', 76), ('NEW', 73), ('Belgium', 71), ('India', 71), ('Iraq', 68)]\n",
            "MISC: [('Russian', 95), ('Cup', 95), ('German', 84), ('British', 78), ('Open', 71), ('French', 67), ('European', 55), ('American', 55), ('Australian', 52), ('Iraqi', 51), ('Dutch', 50), ('Israeli', 49), ('GMT', 49), ('World', 49), ('League', 49), ('DIVISION', 48), ('African', 45), ('English', 42), ('Olympic', 41), ('LEAGUE', 41)]\n"
          ]
        }
      ],
      "source": [
        "# Show the 20 most common words in each tag class.\n",
        "for tag, examples in tag_word_examples.items():\n",
        "    print(f'{tag}: {list(examples.most_common())[:20]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjydSYOPyhaN"
      },
      "source": [
        "## MEMM Model Setup\n",
        "While the MEMM model can be highly flexible in feature representations, here we'll demonstrate a somewhat limited case. We'll construct an MEMM with two types of features; binary features when a word matches a regular expression ([1] if it's a match, otherwise [0]), and a one-hot encoding for prior tags (e.g. [0, 0, 1, 0, 0] for a tag associated with id 2).\n",
        "\n",
        "Since the MEMM is not constrained to use only single state-transitions and single-observations, we'll construct the MEMM with a word-context window of 8 (4 on each side) and we'll include the prior 5 tags as features.\n",
        "\n",
        "$$\\hat{s}_i = \\text{argmax}_{s} P(s|o_{i+4}, \\dots, o_i, \\dots, o_{i-4}, s_{i-1}, \\dots s_{i-5})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzxxqXVIyiX6"
      },
      "source": [
        "Below is the list of regular expressions we use to generate features for each word in the context window.\n",
        "Each regular expression corresponds to a feature in the model's input representation for each word in the context window."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLQxiRtszIiL"
      },
      "source": [
        "You can use [Regex101](https://regex101.com/r/WrtvQl/1) to debug your regular expression (make sure to set it to Python version).\n",
        "For a quick refersher on regex, see [regexp](https://users.cs.cf.ac.uk/Dave.Marshall/Internet/NEWS/regexp.html) or the [python re documentation](https://docs.python.org/3/library/re.html#regular-expression-syntax)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9130Ql_IyeuD"
      },
      "outputs": [],
      "source": [
        "regex_features = {\n",
        "    # 'feature name': r'pattern',  # intuition for these features\n",
        "    'ENDS_IN_AN': r'(an|AN)$',  # words like Samoan, INDIAN, etc.\n",
        "    'LEN_TWO_AND_CAPS': r'^[A-Z]{2}$',  # Locations like CA, UK, etc.\n",
        "    'IS_PERIOD': r'^\\.$',  # . is common and usually O\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is9ONk5X9i9Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "outputId": "b3539910-b3d9-46a0-b1da-e0942d63f86c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/14041 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-1df41b8f16f9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m runMEMM(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_corpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtest_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_corpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdecode_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'greedy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-26f7e2e5697e>\u001b[0m in \u001b[0;36mrunMEMM\u001b[0;34m(train_corpus, test_corpus, dic, decode_type, regex_features, rear_context_window, forward_context_window, num_prior_tags, output_file)\u001b[0m\n\u001b[1;32m     12\u001b[0m                  \u001b[0mrear_context_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrear_context_window\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                  forward_context_window=forward_context_window, num_prior_tags=num_prior_tags)\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train results:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2c69c5fef2ec>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Extracting features...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_features_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2c69c5fef2ec>\u001b[0m in \u001b[0;36mget_features_labels\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2c69c5fef2ec>\u001b[0m in \u001b[0;36mextract_feature\u001b[0;34m(self, words, tags, i)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# (6 points)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# START HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mtag_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;31m# END\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;34m\"infrequent_if_exist\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         }\n\u001b[0;32m--> 917\u001b[0;31m         X_int, X_mask = self._transform(\n\u001b[0m\u001b[1;32m    918\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         X_list, n_samples, n_features = self._check_X(\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X, force_all_finite)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ndim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# if not a dataframe, do normal check_array validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required."
          ]
        }
      ],
      "source": [
        "runMEMM(\n",
        "    train_corpus = train_corpus,\n",
        "    test_corpus = test_corpus,\n",
        "    dic = dic,\n",
        "    decode_type = 'greedy',\n",
        "    regex_features=regex_features,\n",
        "    rear_context_window=4,\n",
        "    forward_context_window=4,\n",
        "    num_prior_tags=5,\n",
        "    output_file = None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny-w49tcvzwn"
      },
      "source": [
        "## Add Regex features for problem 2(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8bMdWBNyokD"
      },
      "outputs": [],
      "source": [
        "regex_features_2 = {\n",
        "    'ENDS_IN_AN': r'(an|AN)$', # words like Samoan, INDIAN, etc.\n",
        "    'LEN_TWO_AND_CAPS': r'^[A-Z]{2}$', # Locations like CA, UK, etc.\n",
        "    'IS_PERIOD': r'^\\.$', # . is common and usually O\n",
        "\n",
        "    # New regular expression features\n",
        "    'STARTS_WITH_CAPS': r'^[A-Z]', # Words starting with a capital letter\n",
        "    'ENDS_WITH_S': r's$', # Words ending with \"s\"\n",
        "    'CONTAINS_DIGIT': r'\\d', # Words containing a digit\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQ2hwEN2yoEy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "outputId": "692de372-1336-4050-cb1c-b8be5baf1128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/14041 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-dce2bd61fe86>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m runMEMM(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_corpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtest_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_corpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdecode_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'greedy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-26f7e2e5697e>\u001b[0m in \u001b[0;36mrunMEMM\u001b[0;34m(train_corpus, test_corpus, dic, decode_type, regex_features, rear_context_window, forward_context_window, num_prior_tags, output_file)\u001b[0m\n\u001b[1;32m     12\u001b[0m                  \u001b[0mrear_context_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrear_context_window\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                  forward_context_window=forward_context_window, num_prior_tags=num_prior_tags)\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train results:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2c69c5fef2ec>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Extracting features...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_features_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2c69c5fef2ec>\u001b[0m in \u001b[0;36mget_features_labels\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2c69c5fef2ec>\u001b[0m in \u001b[0;36mextract_feature\u001b[0;34m(self, words, tags, i)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# (6 points)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# START HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mtag_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;31m# END\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;34m\"infrequent_if_exist\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         }\n\u001b[0;32m--> 917\u001b[0;31m         X_int, X_mask = self._transform(\n\u001b[0m\u001b[1;32m    918\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         X_list, n_samples, n_features = self._check_X(\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X, force_all_finite)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ndim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# if not a dataframe, do normal check_array validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required."
          ]
        }
      ],
      "source": [
        "runMEMM(\n",
        "    train_corpus = train_corpus,\n",
        "    test_corpus = test_corpus,\n",
        "    dic = dic,\n",
        "    decode_type = 'greedy',\n",
        "    regex_features=regex_features_2,\n",
        "    rear_context_window=4,\n",
        "    forward_context_window=4,\n",
        "    num_prior_tags=5,\n",
        "    output_file = None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO (5 points)\n",
        "\n",
        "Please write down in the **text cell** your observations about how the extra regex features affect performance? Which ones help the most and which ones don't help much? Why do you think that is the case?\n",
        "\n",
        "(You don't need to add this to the pdf write-up.)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a0ahxNLTQWPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2vCeJmMihro2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "py38",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "64bcadabe4cd61f3d117ba0da9d14bf2f8e35582ff79e821f2e71056f2723d1e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}